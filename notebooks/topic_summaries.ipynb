{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added source: PKM\n",
      "Added source: IHAMS\n",
      "Added source: Balance and Decisions\n",
      "Added source: Living Knowledge Management\n",
      "Added source: Mission Control\n",
      "Added source: Obsidian Journaling Product\n",
      "6 documents loaded with a total of 58,554 words.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import ObsidianLoader\n",
    "from pathlib import Path\n",
    "\n",
    "path = \"/home/codyt/Documents/Personal/Research\"\n",
    "loader = ObsidianLoader(path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract source from metadata and ensure it's properly set\n",
    "for doc in docs:\n",
    "    source_path = Path(doc.metadata.get('source', ''))\n",
    "    doc.metadata['source'] = source_path.stem\n",
    "    print(f\"Added source: {doc.metadata['source']}\")  # Debug print\n",
    "\n",
    "# Calculate the number of words total for all documents\n",
    "total_words = sum(len(doc.page_content.split()) for doc in docs)\n",
    "\n",
    "# Print the number of documents and total words\n",
    "print(f\"{len(docs)} documents loaded with a total of {total_words:,} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Load the API key from the .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# PPLX_API_KEY = os.getenv(\"PPLX_API_KEY\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.chat_models import ChatPerplexity\n",
    "\n",
    "\n",
    "fast_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# external_knowledge_model = ChatPerplexity(model=\"llama-3.1-sonar-large-128k-online\")\n",
    "large_model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 33 documents.\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['generate_summary']\n",
      "['collect_summaries']\n",
      "['generate_final_summary']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "knowledge_prompt = \"\"\"\n",
    "You are an expert document analyzer tasked with identifying and describing the developmental process of ideas in a structured mini-narrative format on {topic}.\n",
    "\n",
    "**Task:**  \n",
    "1. Analyze the text to create a mini-narrative that describes:  \n",
    "   - What is being discussed in the document, including the central idea(s).  \n",
    "   - What happens throughout the document, including any key developments, changes, or shifts in the idea(s).  \n",
    "   - How the document progresses from beginning to middle to end, focusing on the natural flow of the developmental process.  \n",
    "2. Only include motivations when they are explicitly evident as playing a role in the developmental process. Avoid speculating or forcing interpretations of intent.  \n",
    "3. Refrain from making introductions, conclusions, or asking further questions about the document. Simply narrate the developmental process based on the provided text.\n",
    "\n",
    "**Output Format:**  \n",
    "Provide a structured mini-narrative with these elements:  \n",
    "- **What is being discussed:** Clearly state the central idea(s) or topic(s) at the core of the document.  \n",
    "- **What happens:** Describe the key points and developments as they progress throughout the document. Focus on identifying changes, shifts, or refinements in the ideas.  \n",
    "- **Natural Progression:** Narrate the flow of the document from beginning to middle to end, emphasizing the natural evolution of the ideas.  \n",
    "\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "You are an expert in synthesizing and consolidating information to create a cohesive narrative.  \n",
    "\n",
    "**Context:**  \n",
    "The following are summaries of key developments related to the evolution of an idea:  \n",
    "{docs}  \n",
    "\n",
    "**Task:**  \n",
    "1. Carefully analyze the provided summaries to identify the progression and transformation of the idea over time.  \n",
    "2. Combine these summaries into a single, cohesive narrative that:  \n",
    "   - Clearly explains how the central idea evolved.  \n",
    "   - Highlights key developments, shifts, and refinements in the idea across the summaries.  \n",
    "   - Maintains logical flow and clarity throughout.  \n",
    "3. Use a natural storytelling structure that captures the progression from beginning to end, while avoiding repetition or introducing new information.  \n",
    "\n",
    "**Output Format:**  \n",
    "Provide a structured, consolidated narrative with the following elements:  \n",
    "- **Introductory Context:** Briefly introduce the central idea and scope of the summaries.  \n",
    "- **Evolution of the Idea:** Narrate how the idea progressed or transformed over time, detailing the key developments in a logical manner.  \n",
    "- **Final State:** Conclude the narrative by summarizing the final state or refined version of the idea based on the summaries provided.\n",
    "\"\"\"\n",
    "\n",
    "# Create the map prompt with the knowledge item template\n",
    "map_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", knowledge_prompt),\n",
    "    (\"user\", \"Write a concise summary of the following:\\n\\n{content}\")\n",
    "])\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=3200, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(split_docs)} documents.\")\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, List, Literal, TypedDict, Dict, Any\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import (\n",
    "    acollapse_docs,\n",
    "    split_list_of_docs,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "token_max = 16000\n",
    "\n",
    "\n",
    "def length_function(documents: List[Document]) -> int:\n",
    "    \"\"\"Get number of tokens for input contents.\"\"\"\n",
    "    return sum(fast_model.get_num_tokens(doc.page_content) for doc in documents)\n",
    "\n",
    "\n",
    "# This will be the overall state of the main graph.\n",
    "# It will contain the input document contents, corresponding\n",
    "# summaries, and a final summary.\n",
    "class OverallState(TypedDict):\n",
    "    # Notice here we use the operator.add\n",
    "    # This is because we want combine all the summaries we generate\n",
    "    # from individual nodes back into one list - this is essentially\n",
    "    # the \"reduce\" part\n",
    "    contents: List[str]\n",
    "    summaries: Annotated[list, operator.add]\n",
    "    sources: Annotated[List[str], operator.add]\n",
    "    collapsed_summaries: List[Document]\n",
    "    final_summary: str\n",
    "\n",
    "\n",
    "# This will be the state of the node that we will \"map\" all\n",
    "# documents to in order to generate summaries\n",
    "class SummaryState(TypedDict):\n",
    "    content: str\n",
    "    topic: str\n",
    "    source: str\n",
    "\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute=10):  # More conservative limit\n",
    "        self.calls_per_minute = calls_per_minute\n",
    "        self.calls = []\n",
    "        self.last_error_time = None\n",
    "        \n",
    "    async def wait(self):\n",
    "        now = datetime.now()\n",
    "        # Add exponential backoff after errors\n",
    "        if self.last_error_time and (now - self.last_error_time) < timedelta(minutes=2):\n",
    "            await asyncio.sleep(15)  # Longer cooldown after errors\n",
    "            \n",
    "        self.calls = [call for call in self.calls if now - call < timedelta(minutes=1)]\n",
    "        if len(self.calls) >= self.calls_per_minute:\n",
    "            wait_time = 60 - (now - self.calls[0]).total_seconds() + 5  # Added buffer\n",
    "            if wait_time > 0:\n",
    "                await asyncio.sleep(wait_time)\n",
    "        self.calls.append(now)\n",
    "\n",
    "# Create rate limiter instance\n",
    "rate_limiter = RateLimiter(calls_per_minute=45)\n",
    "\n",
    "from openai import RateLimitError  # Add this import at the top\n",
    "\n",
    "# Modify the generate_summary function\n",
    "async def generate_summary(state: SummaryState):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            await rate_limiter.wait()\n",
    "            prompt = map_prompt.invoke({\n",
    "                \"content\": state[\"content\"],\n",
    "                \"topic\": state[\"topic\"]\n",
    "            })\n",
    "            response = await fast_model.ainvoke(prompt)\n",
    "            return {\n",
    "                \"summaries\": [response.content],\n",
    "                \"sources\": [state[\"source\"]]\n",
    "            }\n",
    "        except RateLimitError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait_time = (attempt + 1) * 30\n",
    "            print(f\"Rate limit exceeded. Waiting {wait_time} seconds before retry...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "\n",
    "\n",
    "# Here we define the logic to map out over the documents\n",
    "# We will use this an edge in the graph\n",
    "def map_summaries(state: OverallState):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that node\n",
    "     return [\n",
    "        Send(\"generate_summary\", {\n",
    "            \"content\": doc.page_content,\n",
    "            \"topic\": state.get(\"topic\", \"Default Topic\"),\n",
    "            \"source\": Path(doc.metadata.get('source', '')).stem or f'doc_{i}'\n",
    "        }) for i, doc in enumerate(state[\"contents\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def collect_summaries(state: OverallState):\n",
    "    # Deduplicate sources while maintaining order\n",
    "    unique_sources = list(dict.fromkeys(state[\"sources\"]))\n",
    "    return {\n",
    "        \"collapsed_summaries\": [\n",
    "            Document(\n",
    "                page_content=summary,\n",
    "                metadata={\"source\": source}\n",
    "            ) \n",
    "            # Change this line to use unique_sources instead of state[\"sources\"]\n",
    "            for summary, source in zip(state[\"summaries\"], unique_sources)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "async def _reduce(input: dict) -> str:\n",
    "    prompt = reduce_prompt.invoke(input)\n",
    "    response = await large_model.ainvoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Add node to collapse summaries\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    doc_lists = split_list_of_docs(\n",
    "        state[\"collapsed_summaries\"], length_function, token_max\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in doc_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "\n",
    "    return {\"collapsed_summaries\": results}\n",
    "\n",
    "\n",
    "# This represents a conditional edge in the graph that determines\n",
    "# if we should collapse the summaries or not\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = length_function(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > token_max:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Here we will generate the final summary\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    # Flatten and deduplicate sources from collapsed summaries\n",
    "    all_sources = []\n",
    "    for doc in state[\"collapsed_summaries\"]:\n",
    "        if isinstance(doc, Document):\n",
    "            all_sources.append(doc.metadata[\"source\"])\n",
    "        elif isinstance(doc, list):\n",
    "            all_sources.extend(d.metadata[\"source\"] for d in doc)\n",
    "    \n",
    "    # Deduplicate and sort sources\n",
    "    unique_sources = sorted(dict.fromkeys(all_sources))\n",
    "    \n",
    "    # Generate summary\n",
    "    collapsed_content = []\n",
    "    for doc in state[\"collapsed_summaries\"]:\n",
    "        if isinstance(doc, Document):\n",
    "            collapsed_content.append(doc.page_content)\n",
    "        elif isinstance(doc, list):\n",
    "            collapsed_content.extend(d.page_content for d in doc)\n",
    "    \n",
    "    summary = await _reduce({\n",
    "        \"docs\": \"\\n\\n\".join(collapsed_content)\n",
    "    })\n",
    "    \n",
    "    source_list = \"\\n\".join(f\"- [[{source}]]\" for source in unique_sources)\n",
    "    \n",
    "    return {\n",
    "        \"final_summary\": f\"{summary}\\n\\n## Sources\\n{source_list}\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "async for step in app.astream(\n",
    "    {\n",
    "        \"contents\": split_docs,\n",
    "        \"topic\": \"A self improving OS\"        \n",
    "    },\n",
    "    {\"recursion_limit\": 30},\n",
    "):\n",
    "    \n",
    "    print(list(step.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Introductory Context:**\n",
       "\n",
       "The central idea explored in these summaries revolves around the optimization of knowledge management and Retrieval-Augmented Generation (RAG) through strategic organization and AI integration. This involves the evolution of note structuring, vectorization strategies, goal management, and AI-driven journaling, with the aim of enhancing information retrieval, personal growth, and business success.\n",
       "\n",
       "**Evolution of the Idea:**\n",
       "\n",
       "Initially, the focus was on comparing two vectorization strategies for RAG: large topic-based files and separate files for subtopics. The discussion highlighted the benefits of dense contextual information from large files versus the retrieval efficiency and granularity offered by separate files. As the exploration deepened, the preference shifted towards separate files for subtopics, offering enhanced retrieval efficiency and relevance. This shift marked a pivotal moment in optimizing vectorization processes for Personal Knowledge Management (PKM) systems.\n",
       "\n",
       "Simultaneously, the framework for organizing knowledge management was established, emphasizing the importance of structured notes, tagging, and metadata. This foundation allowed for the breakdown of complex concepts, like communication, into sub-concepts, enhancing searchability and usability. The discussion evolved towards practical applications, recommending folder organization and integration of project management into the knowledge system, thereby streamlining the management of goals and tasks.\n",
       "\n",
       "The narrative then moved to goal setting and management within a note-taking system, where each goal was delineated into its own note with projects as subsections. This structure fostered clarity and motivation by connecting projects to overarching objectives. The exploration extended to technical aspects of RAG, such as batch size and parameter settings, underpinning the need for empirical testing to optimize model performance.\n",
       "\n",
       "Parallelly, the integration of AI in journaling products was introduced, transforming journaling into a more interactive, insightful experience. The focus was on enhancing AI prompts to ensure empathetic, personalized feedback based on users' journal entries. This development underscored the potential of AI to analyze emotions and track personal growth, enriching the journaling experience.\n",
       "\n",
       "**Final State:**\n",
       "\n",
       "The culmination of these developments resulted in a sophisticated system that integrates efficient vectorization, structured knowledge management, goal-oriented project planning, and AI-enhanced journaling. The final state of the idea is a cohesive framework that leverages strategic organization and AI capabilities to improve information retrieval, personal development, and business outcomes. This integrated approach signifies a mature, refined solution that addresses the complexities of managing and utilizing personal and professional knowledge in an increasingly digital world.\n",
       "\n",
       "## Sources\n",
       "- [[Balance and Decisions]]\n",
       "- [[IHAMS]]\n",
       "- [[Living Knowledge Management]]\n",
       "- [[Mission Control]]\n",
       "- [[Obsidian Journaling Product]]\n",
       "- [[PKM]]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Access nested value\n",
    "display(Markdown(step['generate_final_summary']['final_summary']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
